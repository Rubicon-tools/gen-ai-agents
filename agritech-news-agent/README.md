# ğŸ§  Agritech News Agent

A robust and resumable scraper that collects agriculture-related research papers from [arXiv.org](https://arxiv.org). Built with Docker, PostgreSQL, and Python â€” optimized for performance, duplicate handling, and future-proof extensions (e.g., PDF storage in DigitalOcean).

---

## ğŸš€ Features

- ğŸ” Scrapes agriculture-related articles from arXiv (using search).
- âœ… Automatically skips already-scraped articles when using `--continue` mode.
- ğŸ§  Resumable scraping using `article_id` logic.
- ğŸ“¦ Saves all metadata into a PostgreSQL database.
- âš™ï¸ Dockerized for easy local or remote deployments.

---

## ğŸ“ Project Structure

```bash
agritech-news-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ scraper.py         # Scraping logic and logic for resume mode
â”‚   â”œâ”€â”€ db.py              # DB connection, schema, and insert logic
â”‚   â”œâ”€â”€ config.py          # Environment-based settings
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ Dockerfile             # Container build instructions
â”œâ”€â”€ docker-compose.yml     # Services config (Postgres, scraper)
â”œâ”€â”€ docker.sh              # CLI tool to build/start/scrape
â”œâ”€â”€ main.py                # Entry point for the scraper
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Youâ€™re reading it!
```

---

## âš™ï¸ Setup Instructions

### 1. ğŸ“¦ Install Dependencies

Make sure you have:

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- (Optional) [Python 3.11+](https://www.python.org/) if running outside Docker

---

### 2. âš™ï¸ Configure Environment

Create a `.env` file with the following content:

```env
ENV=dev

POSTGRES_HOST=n8n-builder-dev-postgres
POSTGRES_PORT=5432
POSTGRES_USER=n8n
POSTGRES_PASSWORD=your_postgres_password
POSTGRES_DB=n8n_db
```

---

### 3. ğŸ³ Build & Start the App

```bash
bash docker.sh build
```

This builds the image and starts the container. To check if itâ€™s running:

```bash
docker ps
```

---

## ğŸ§ª Usage

### â• Scrape New Articles

```bash
bash docker.sh scrape
```

This will scrape the latest 10 articles and **stop on the first duplicate** found.

### ğŸ” Continue Mode (Skip Existing)

```bash
bash docker.sh scrape -continue
```

Keeps scraping up to 10 articles, **skipping** those already saved; usually used when your scraper stopped mid-scrape.

### ğŸ¯ Specify Limit

```bash
bash docker.sh scrape 20
bash docker.sh scrape 20 -continue
```

---

## ğŸ§¼ Cleanup

To stop and remove containers:

```bash
bash docker.sh stop
```

---

## ğŸ“„ License

MIT License â€” do whatever you want, but give credit where due.

---
## âœ¨ Credits

Built by the GenAI team ğŸ§  with â¤ï¸ â€” Powered by Docker & Python