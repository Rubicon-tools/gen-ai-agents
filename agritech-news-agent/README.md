# ğŸ§  Agritech News Agent

A robust and resumable agent that collects agriculture-related research papers from [arXiv.org](https://arxiv.org).  
Built with Docker, PostgreSQL, and Python â€” optimized for performance, duplicate handling, and future-proof extensions (e.g., PDF storage in DigitalOcean Spaces).

---

## âš™ï¸ Initial Setup â€” One-Time Bootstrap

Before building this project, you must first set up the shared base infrastructure:

1. **Go to the `gen-ai-agents` folder** (the root folder for all subprojects)
2. **Run the build script there** to create shared services and network:

```bash
cd gen-ai-agents
bash docker.sh build
```

This will create the necessary shared containers and network:

- ğŸ§  `gen-ai-caddy` â€” reverse proxy
- ğŸŒ `gen-ai-network` â€” the shared Docker network used by all sub-projects

> âš ï¸ **Important:** This step only needs to be done **once**. All subprojects (like `agritech-news-agent`) will then attach to this shared `gen-ai-network`.

3. **Adapt your `.env`** in the `agritech-news-agent` folder to point to the shared PostgreSQL service (`POSTGRES_HOST`, etc.)

4. Now you can build this project:

```bash
cd agritech-news-agent
bash docker.sh build
```

---

## ğŸš€ Features

- âš¡ Scrape or fetch agriculture-related articles from arXiv
- ğŸ§  Two ingestion modes:
  - `scraper/` â€” legacy HTML scraper
  - `api_scraper/` â€” modern API-based fetcher (recommended)
- âœ… Automatically skips already-inserted articles
- ğŸ“¦ Saves metadata to PostgreSQL
- â˜ï¸ Optionally uploads PDFs to DigitalOcean Spaces
- âš™ï¸ Fully Dockerized

---

## ğŸ“ Project Structure

```bash
agritech-news-agent/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ uploader.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ scraper/            # ğŸ•¸ï¸ Legacy HTML scraper
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api.py
â”‚   â”‚   â”œâ”€â”€ scraper.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”‚
â”‚   â”œâ”€â”€ api_scraper/         # âš¡ Official API-based fetcher
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api.py
â”‚   â”‚   â”œâ”€â”€ scraper.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”‚
â”‚   â””â”€â”€ rag/                 # Optional RAG ingestion logic
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ api.py
â”‚
â”œâ”€â”€ docker.sh
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose-dev.yml
â”œâ”€â”€ docker-compose-prod.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## âš™ï¸ Environment Variables

```env
ENV=dev

POSTGRES_HOST=n8n-builder-dev-postgres
POSTGRES_PORT=5432
POSTGRES_USER=n8n
POSTGRES_PASSWORD=your_postgres_password
POSTGRES_DB=n8n_db

S3_UPLOAD=false
SPACES_KEY=your_access_key
SPACES_SECRET=your_secret_key
SPACES_REGION=fra1
SPACES_BUCKET=your-bucket-name
SPACES_ENDPOINT=https://fra1.digitaloceanspaces.com
```

---

ğŸ“‹ Available Commands

| Command | Description | Example |
|---|---|---|
| `build` | Build the image with the selected compose file and start containers | `bash docker.sh build` |
| `up` | Start containers (detached) using the selected compose file | `bash docker.sh up` |
| `restart` | Restart running containers | `bash docker.sh restart` |
| `down` | Stop and remove containers | `bash docker.sh down` |
| `logs [service]` | Show container logs (optionally for a specific service) | `bash docker.sh logs app` |
| `scrape [limit] [-continue] [-bg]` | Run the **legacy HTML** scraper. `limit` defaults to 25; `-continue` skips already-saved articles; `-bg` runs in background and writes to `logs/scraper.out`. | `bash docker.sh scrape 50 -continue -bg` |
| `scrape-newest [-bg]` | Run the **legacy HTML** scraper for newest items (uses `--newest --continue`). `-bg` runs in background to `logs/scraper-newest.out`. | `bash docker.sh scrape-newest -bg` |
| `api-scraper [-bg] [-q|--query q] [--page-size N] [--sleep S] [--total T] [--start-page P]` | Run the **API** scraper in **oldestâ†’newest (single pass)** mode. Defaults: `q=agriculture`, `page-size=200`, `sleep=3.0`. Use `--total` to supply known total results and `--start-page` (1-based) to resume. `-bg` logs to `logs/api-scraper.out`. | `bash docker.sh api-scraper --total 3179 --page-size 200` |
| `api-scraper-newest [-bg] [-q|--query q] [--page-size N] [--sleep S] [--newest-pages K]` | Run the **API** scraper for the newest window (default last **3 pages**). Inserts in chronological order and skips duplicates. `-bg` logs to `logs/api-scraper-newest.out`. | `bash docker.sh api-scraper-newest --newest-pages 3 -bg` |
| `stop-scraper` | Stop any background scraper process running inside the container | `bash docker.sh stop-scraper` |

---

## ğŸ§ª Usage

### ğŸ•¸ï¸ Legacy HTML Scraper

#### Full Backfill (Oldest â†’ Newest)

```bash
bash docker.sh scrape
bash docker.sh scrape 20 -continue
```

- Scrapes using the HTML pages  
- `-continue` skips already-saved IDs  
- Use `-bg` to run in background  
- Logs: `logs/scraper.out`

#### Daily Sync (Newest-Only Window)

```bash
bash docker.sh scrape-newest
bash docker.sh scrape-newest 20 -continue
```

- Skips duplicates automatically  
- Logs: `logs/scraper-newest.out`

---

### âš¡ API Scraper (Recommended)

Fetches directly from the official arXiv API â€” faster, more reliable, and version-safe.

#### Full Backfill (Oldest â†’ Newest)

```bash
bash docker.sh api-scraper --total 3179 --page-size 200
```

- Downloads all articles (starting from oldest)  
- Use `-bg` to run in background  
- Logs: `logs/api-scraper.out`

#### Daily Sync (Newest-Only Window)

```bash
bash docker.sh api-scraper-newest --newest-pages 3 --page-size 200
```

- Checks only the newest ~600 articles  
- Skips duplicates automatically  
- Logs: `logs/api-scraper-newest.out`

---

## ğŸ§¼ Cleanup

Stop and remove containers:

```bash
bash docker.sh down
```

---

## ğŸ“„ License

MIT â€” free to use, modify, and distribute.
