# ğŸ§  Agritech News Agent

A robust and resumable scraper that collects agriculture-related research papers from [arXiv.org](https://arxiv.org). Built with Docker, PostgreSQL, and Python â€” optimized for performance, duplicate handling, and future-proof extensions (e.g., PDF storage in DigitalOcean).

---

## ğŸš€ Features

- ğŸ” Scrapes agriculture-related articles from arXiv (using search).
- âœ… Automatically skips already-scraped articles when using `--continue` mode.
- ğŸ§  Resumable scraping using `article_id` logic.
- ğŸ“¦ Saves all metadata into a PostgreSQL database.
- âš™ï¸ Dockerized for easy local or remote deployments.

---

## ğŸ“ Project Structure

```bash
agritech-news-agent/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api.py
â”‚   â”‚   â”œâ”€â”€ scraper.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â””â”€â”€ rag/                     
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ api.py
â”œâ”€â”€ Dockerfile.app
â”œâ”€â”€ Dockerfile.web
â”œâ”€â”€ docker-compose-dev.yml
â”œâ”€â”€ docker-compose-prod.yml
â”œâ”€â”€ docker.sh
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## âš™ï¸ Setup Instructions

### 1. ğŸ“¦ Install Dependencies

Make sure you have:

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- (Optional) [Python 3.11+](https://www.python.org/) if running outside Docker

---

### 2. âš™ï¸ Configure Environment

Create a `.env` file with the following content:

```env
ENV=dev

POSTGRES_HOST=n8n-builder-dev-postgres
POSTGRES_PORT=5432
POSTGRES_USER=n8n
POSTGRES_PASSWORD=your_postgres_password
POSTGRES_DB=n8n_db
```

---

### 3. ğŸ³ Build & Start the App

```bash
bash docker.sh build
```

This builds the image and starts the container. To check if itâ€™s running:

```bash
docker ps
```

---

## ğŸ§ª Usage

### â• Scrape New Articles

```bash
bash docker.sh scrape
```

This will scrape the latest 10 articles and **stop on the first duplicate** found.

### ğŸ” Continue Mode (Skip Existing)

```bash
bash docker.sh scrape -continue
```

Keeps scraping up to 10 articles, **skipping** those already saved; usually used when your scraper stopped mid-scrape.

### ğŸ¯ Specify Limit

```bash
bash docker.sh scrape 20
bash docker.sh scrape 20 -continue
```

#### ğŸ§© Run in Background (usually used in production)

To run the scraper in the background (using `nohup`):

```bash
bash docker.sh scrape -bg
bash docker.sh scrape 20 -continue -bg
```

This will run the job in background and save logs to nohup.out.

### âŒ Kill Background Job (usually used in production)

To stop the background scraper:

```bash
bash docker.sh stop-scraper
```

This will stop the background scraper.

---

## ğŸ§¼ Cleanup

To stop and remove containers:

```bash
bash docker.sh stop
```

---

## ğŸ“„ License

MIT License â€” do whatever you want, but give credit where due.

---
## âœ¨ Credits

Built by the GenAI team ğŸ§  with â¤ï¸ â€” Powered by Docker & Python