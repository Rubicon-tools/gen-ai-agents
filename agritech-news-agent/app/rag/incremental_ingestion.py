#!/usr/bin/env python3
"""
Pipeline d'ingestion incr√©mentale pour le syst√®me RAG
====================================================

Ce script traite uniquement les NOUVEAUX documents PDF et les ajoute
√† la base vectorielle existante, sans dupliquer les documents d√©j√† index√©s.

Utilisation:
- Premier lancement: traite tous les PDFs
- Lancements suivants: traite seulement les nouveaux PDFs
"""

import warnings
warnings.filterwarnings("ignore", message=".*urllib3.*OpenSSL.*")

import os
import sys
import json
from typing import List, Dict, Any

# Import des modules
from modules.ingestion import load_pdfs_from_folder
from modules.chunking import split_texts_into_chunks
from modules.embeddings import embed_texts
from modules.vectorstore import (
    get_qdrant_client,
    get_existing_documents,
    upsert_embeddings_incremental,
    get_document_hash
)

class IncrementalIngestionPipeline:
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.qdrant_client = get_qdrant_client()
        self.collection_name = "rag_collection"
        
    def get_pdf_files_with_hashes(self) -> Dict[str, str]:
        """R√©cup√®re tous les fichiers PDF avec leurs hashes."""
        pdf_files = {}
        for filename in sorted(os.listdir(self.data_dir)):
            if filename.lower().endswith('.pdf'):
                file_path = os.path.join(self.data_dir, filename)
                file_hash = get_document_hash(file_path)
                pdf_files[filename] = file_hash
        return pdf_files
    
    def identify_new_documents(self) -> List[str]:
        """Identifie les documents qui n'ont pas encore √©t√© index√©s."""
        print("üîç V√©rification des documents existants...")
        
        # R√©cup√©rer les documents d√©j√† index√©s
        existing_docs = get_existing_documents(self.qdrant_client)
        existing_hashes = set(existing_docs.keys())
        
        # R√©cup√©rer tous les PDFs avec leurs hashes
        all_pdfs = self.get_pdf_files_with_hashes()
        
        # Identifier les nouveaux documents
        new_documents = []
        for filename, file_hash in all_pdfs.items():
            if file_hash not in existing_hashes:
                new_documents.append(filename)
            else:
                print(f"‚úÖ {filename} - d√©j√† index√©")
        
        return new_documents
    
    def process_single_document(self, filename: str) -> Dict[str, Any]:
        """Traite un seul document PDF."""
        file_path = os.path.join(self.data_dir, filename)
        file_hash = get_document_hash(file_path)
        
        print(f"\nüìÑ Traitement de: {filename}")
        print(f"üîê Hash: {file_hash[:8]}...")
        
        # Extraire le contenu
        try:
            texts = load_pdfs_from_folder(self.data_dir, max_files=None)
            # Filtrer pour ne garder que le document actuel
            current_text = None
            for text in texts:
                # V√©rifier si ce texte correspond au fichier actuel
                # (approximation bas√©e sur la taille du fichier)
                if len(text) > 1000:  # Filtre basique
                    current_text = text
                    break
            
            if not current_text:
                raise ValueError(f"Aucun contenu extrait de {filename}")
            
            # D√©couper en chunks
            chunks = split_texts_into_chunks([current_text])
            
            # G√©n√©rer les embeddings
            embeddings = embed_texts(chunks)
            
            # Ajouter √† la base vectorielle
            document_hashes = [file_hash] * len(chunks)
            next_id = upsert_embeddings_incremental(
                self.qdrant_client, 
                embeddings, 
                chunks, 
                document_hashes
            )
            
            return {
                "filename": filename,
                "hash": file_hash,
                "chunks_created": len(chunks),
                "embeddings_generated": len(embeddings),
                "next_id": next_id
            }
            
        except Exception as e:
            print(f"‚ùå Erreur lors du traitement de {filename}: {e}")
            return {
                "filename": filename,
                "hash": file_hash,
                "error": str(e)
            }
    
    def run(self) -> Dict[str, Any]:
        """Ex√©cute le pipeline d'ingestion incr√©mentale."""
        print("üîß PIPELINE D'INGESTION INC√âMENTALE RAG")
        print("=" * 50)
        print("Ce script traite uniquement les NOUVEAUX documents PDF.")
        print(f"üìÅ Dossier de donn√©es: {self.data_dir}")
        
        # Identifier les nouveaux documents
        new_documents = self.identify_new_documents()
        
        if not new_documents:
            print("\n‚úÖ Aucun nouveau document √† traiter!")
            print("Tous les PDFs sont d√©j√† index√©s.")
            return {"status": "no_new_documents"}
        
        print(f"\nüÜï {len(new_documents)} nouveau(x) document(s) √† traiter:")
        for doc in new_documents:
            print(f"   ‚Ä¢ {doc}")
        
        # Demander confirmation
        response = input(f"\nüöÄ Traiter ces {len(new_documents)} document(s) ? (y/N): ")
        if response.lower() != 'y':
            print("‚ùå Annul√© par l'utilisateur.")
            return {"status": "cancelled"}
        
        print(f"\nüöÄ D√âMARRAGE DU TRAITEMENT INC√âMENTAL")
        print("=" * 50)
        
        results = {
            "status": "success",
            "new_documents": len(new_documents),
            "processed_documents": [],
            "errors": []
        }
        
        # Traiter chaque nouveau document
        for i, filename in enumerate(new_documents, 1):
            print(f"\nüìÑ [{i}/{len(new_documents)}] Traitement de: {filename}")
            
            result = self.process_single_document(filename)
            
            if "error" in result:
                results["errors"].append(result)
                print(f"‚ùå √âchec: {result['error']}")
            else:
                results["processed_documents"].append(result)
                print(f"‚úÖ Succ√®s: {result['chunks_created']} chunks cr√©√©s")
        
        # R√©sum√© final
        print(f"\nüéâ TRAITEMENT INC√âMENTAL TERMIN√â!")
        print("=" * 50)
        print(f"üìä Documents trait√©s: {len(results['processed_documents'])}")
        print(f"‚ùå Erreurs: {len(results['errors'])}")
        
        if results["processed_documents"]:
            total_chunks = sum(doc["chunks_created"] for doc in results["processed_documents"])
            print(f"üß© Total chunks ajout√©s: {total_chunks}")
        
        # Sauvegarder les r√©sultats
        with open("incremental_ingestion_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ R√©sultats sauvegard√©s dans incremental_ingestion_results.json")
        
        return results

def main():
    """Point d'entr√©e principal."""
    if len(sys.argv) > 1:
        data_dir = sys.argv[1]
    else:
        data_dir = "data"
    
    if not os.path.exists(data_dir):
        print(f"‚ùå Le dossier {data_dir} n'existe pas!")
        sys.exit(1)
    
    pipeline = IncrementalIngestionPipeline(data_dir)
    pipeline.run()

if __name__ == "__main__":
    main()
