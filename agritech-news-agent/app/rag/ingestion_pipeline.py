#!/usr/bin/env python3
"""
Pipeline d'ingestion pour le systÃ¨me RAG
========================================

Ce script traite les documents PDF et les indexe dans la base vectorielle.
ExÃ©cutez-le AVANT d'utiliser le systÃ¨me de Q&A.

Ã‰tapes:
1. Extraction des documents PDF
2. DÃ©coupage en chunks optimisÃ©s
3. GÃ©nÃ©ration des embeddings
4. Indexation dans la base vectorielle
"""

import warnings
warnings.filterwarnings("ignore", message=".*urllib3.*OpenSSL.*")

import os
import json
from datetime import datetime
from typing import List, Dict, Any

from modules.ingestion import load_pdfs_from_folder, analyze_extracted_content
from modules.chunking import split_texts_into_chunks, analyze_chunks
from modules.embeddings import embed_texts
from modules.vectorstore import get_qdrant_client, ensure_collection, upsert_embeddings


class IngestionPipeline:
    """Pipeline complet d'ingestion des documents"""
    
    def __init__(self, data_dir: str = "data", collection_name: str = "rag_collection"):
        self.data_dir = data_dir
        self.collection_name = collection_name
        self.qdrant_client = get_qdrant_client()
        
    def run(self) -> Dict[str, Any]:
        """
        ExÃ©cute le pipeline complet d'ingestion
        """
        print("ğŸš€ DÃ‰MARRAGE DU PIPELINE D'INGESTION")
        print("=" * 50)
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "data_directory": self.data_dir,
            "collection_name": self.collection_name,
            "steps": {}
        }
        
        # Ã‰TAPE 1: Extraction des PDFs
        print("\nğŸ“„ Ã‰TAPE 1: Extraction des documents PDF")
        print("-" * 40)
        
        try:
            texts = load_pdfs_from_folder(self.data_dir, max_files=None)  # Traite tous les PDFs
            extraction_analysis = analyze_extracted_content(texts)
            
            results["steps"]["extraction"] = {
                "status": "success",
                "documents_processed": len(texts),
                "analysis": extraction_analysis
            }
            
            print(f"âœ… {len(texts)} documents extraits avec succÃ¨s")
            print(f"ğŸ“Š Total: {extraction_analysis['total_characters']:,} caractÃ¨res")
            
        except Exception as e:
            results["steps"]["extraction"] = {
                "status": "error",
                "error": str(e)
            }
            print(f"âŒ Erreur lors de l'extraction: {e}")
            return results
        
        # Ã‰TAPE 2: DÃ©coupage en chunks
        print(f"\nâœ‚ï¸ Ã‰TAPE 2: DÃ©coupage en chunks optimisÃ©s")
        print("-" * 40)
        
        try:
            chunks = split_texts_into_chunks(
                texts,
                chunk_size_tokens=512,  # Taille fixe
                chunk_overlap_tokens=128  # Overlap significatif
            )
            
            chunk_analysis = analyze_chunks(chunks)
            
            results["steps"]["chunking"] = {
                "status": "success",
                "chunks_created": len(chunks),
                "analysis": chunk_analysis
            }
            
            print(f"âœ… {len(chunks)} chunks crÃ©Ã©s")
            print(f"ğŸ“Š Taille moyenne: {chunk_analysis['avg_size']:.0f} tokens")
            print(f"ğŸ“ˆ Distribution: {chunk_analysis['size_distribution']}")
            
        except Exception as e:
            results["steps"]["chunking"] = {
                "status": "error",
                "error": str(e)
            }
            print(f"âŒ Erreur lors du chunking: {e}")
            return results
        
        # Ã‰TAPE 3: GÃ©nÃ©ration des embeddings
        print(f"\nğŸ§  Ã‰TAPE 3: GÃ©nÃ©ration des embeddings ({len(chunks)} chunks)")
        print("-" * 40)
        
        try:
            chunk_embeddings = embed_texts(chunks)
            
            if not chunk_embeddings or not chunk_embeddings[0]:
                raise RuntimeError("Aucun embedding gÃ©nÃ©rÃ©")
            
            vector_size = len(chunk_embeddings[0])
            
            results["steps"]["embeddings"] = {
                "status": "success",
                "embeddings_generated": len(chunk_embeddings),
                "vector_dimension": vector_size
            }
            
            print(f"âœ… {len(chunk_embeddings)} embeddings gÃ©nÃ©rÃ©s")
            print(f"ğŸ“ Dimension des vecteurs: {vector_size}")
            
        except Exception as e:
            results["steps"]["embeddings"] = {
                "status": "error",
                "error": str(e)
            }
            print(f"âŒ Erreur lors de la gÃ©nÃ©ration des embeddings: {e}")
            return results
        
        # Ã‰TAPE 4: Indexation dans Qdrant
        print(f"\nğŸ—„ï¸ Ã‰TAPE 4: Indexation dans la base vectorielle")
        print("-" * 40)
        
        try:
            # CrÃ©er/recrÃ©er la collection
            ensure_collection(self.qdrant_client, vector_size)
            print(f"âœ… Collection '{self.collection_name}' configurÃ©e")
            
            # Stocker les embeddings
            upsert_embeddings(self.qdrant_client, chunk_embeddings, chunks)
            print(f"âœ… {len(chunks)} chunks indexÃ©s dans Qdrant")
            
            # VÃ©rifier le stockage
            collection_info = self.qdrant_client.get_collection(self.collection_name)
            points_count = self.qdrant_client.count(self.collection_name).count
            
            results["steps"]["indexing"] = {
                "status": "success",
                "collection_status": collection_info.status,
                "points_stored": points_count,
                "vector_dimension": vector_size
            }
            
            print(f"ğŸ“Š Statut de la collection: {collection_info.status}")
            print(f"ğŸ”¢ Points stockÃ©s: {points_count}")
            
        except Exception as e:
            results["steps"]["indexing"] = {
                "status": "error",
                "error": str(e)
            }
            print(f"âŒ Erreur lors de l'indexation: {e}")
            return results
        
        # RÃ‰SUMÃ‰ FINAL
        print("\nğŸ‰ PIPELINE D'INGESTION TERMINÃ‰ AVEC SUCCÃˆS!")
        print("=" * 50)
        
        results["status"] = "success"
        results["summary"] = {
            "total_documents": len(texts),
            "total_chunks": len(chunks),
            "total_embeddings": len(chunk_embeddings),
            "total_points_stored": points_count,
            "vector_dimension": vector_size
        }
        
        print(f"ğŸ“ˆ RÃ‰SUMÃ‰ FINAL:")
        print(f"   â€¢ Documents traitÃ©s: {len(texts)}")
        print(f"   â€¢ Chunks crÃ©Ã©s: {len(chunks)}")
        print(f"   â€¢ Embeddings gÃ©nÃ©rÃ©s: {len(chunk_embeddings)}")
        print(f"   â€¢ Points indexÃ©s: {points_count}")
        print(f"   â€¢ Dimension des vecteurs: {vector_size}")
        
        return results
    
    def save_results(self, results: Dict[str, Any], output_file: str = "ingestion_results.json"):
        """Sauvegarde les rÃ©sultats du pipeline"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ RÃ©sultats sauvegardÃ©s dans {output_file}")
        except Exception as e:
            print(f"âš ï¸  Impossible de sauvegarder les rÃ©sultats: {e}")


def main():
    """Point d'entrÃ©e principal"""
    
    print("ğŸ”§ PIPELINE D'INGESTION RAG")
    print("=" * 40)
    print("Ce script traite les documents PDF et les indexe dans la base vectorielle.")
    print("ExÃ©cutez-le AVANT d'utiliser le systÃ¨me de Q&A.\n")
    
    # VÃ©rifier que le dossier data existe
    data_dir = "data"
    if not os.path.exists(data_dir):
        print(f"âŒ Le dossier '{data_dir}' n'existe pas.")
        print("CrÃ©ez-le et ajoutez-y vos documents PDF.")
        return
    
    # VÃ©rifier qu'il y a des PDFs
    pdf_files = [f for f in os.listdir(data_dir) if f.lower().endswith('.pdf')]
    if not pdf_files:
        print(f"âŒ Aucun fichier PDF trouvÃ© dans '{data_dir}'.")
        print("Ajoutez des documents PDF avant d'exÃ©cuter ce script.")
        return
    
    print(f"ğŸ“ Dossier de donnÃ©es: {data_dir}")
    print(f"ğŸ“„ PDFs trouvÃ©s: {len(pdf_files)}")
    for pdf in pdf_files[:3]:  # Afficher les 3 premiers
        print(f"   â€¢ {pdf}")
    if len(pdf_files) > 3:
        print(f"   â€¢ ... et {len(pdf_files) - 3} autres")
    print()
    
    # Confirmation utilisateur
    response = input("ğŸš€ DÃ©marrer le pipeline d'ingestion ? (y/N): ").strip().lower()
    if response not in ['y', 'yes', 'oui', 'o']:
        print("âŒ Pipeline annulÃ©.")
        return
    
    # ExÃ©cuter le pipeline
    pipeline = IngestionPipeline(data_dir)
    results = pipeline.run()
    
    # Sauvegarder les rÃ©sultats
    if results.get("status") == "success":
        pipeline.save_results(results)
        print("\nâœ… Le systÃ¨me RAG est maintenant prÃªt pour les questions !")
        print("ğŸ’¡ Utilisez 'python main.py' pour poser des questions.")
    else:
        print("\nâŒ Le pipeline a Ã©chouÃ©. VÃ©rifiez les erreurs ci-dessus.")


if __name__ == "__main__":
    main()
