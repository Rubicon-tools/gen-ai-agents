#!/usr/bin/env python3
"""
Ingestion Pipeline for the RAG System
=====================================

This script processes PDF documents and indexes them into the vector database.  
Run it BEFORE using the Q&A system.  

Steps:  
1. Extract PDF documents  
2. Split into optimized chunks  
3. Generate embeddings  
4. Index into the vector database  
"""

import warnings
warnings.filterwarnings("ignore", message=".*urllib3.*OpenSSL.*")

import os
import json
from datetime import datetime
from typing import List, Dict, Any

from modules.ingestion import load_pdfs_from_folder, analyze_extracted_content
from modules.chunking import split_texts_into_chunks, analyze_chunks
from modules.embeddings import embed_texts
from modules.vectorstore import get_qdrant_client, ensure_collection, upsert_embeddings


class IngestionPipeline:
    """Full document ingestion pipeline"""
    
    def __init__(self, data_dir: str = "data", collection_name: str = "rag_collection"):
        self.data_dir = data_dir
        self.collection_name = collection_name
        self.qdrant_client = get_qdrant_client()
        
    def run(self) -> Dict[str, Any]:
        """
        Run the full ingestion pipeline
        """
        print("ğŸš€ STARTING INGESTION PIPELINE")
        print("=" * 50)
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "data_directory": self.data_dir,
            "collection_name": self.collection_name,
            "steps": {}
        }
        
            # Ã‰TAPE 1: Extraction des PDFs
        print("\nğŸ“„ STEP 1: Extraction des documents PDF")
        print("-" * 40)
        
        try:
            texts = load_pdfs_from_folder(self.data_dir, max_files=None)  # Traite tous les PDFs
            extraction_analysis = analyze_extracted_content(texts)
            
            results["steps"]["extraction"] = {
                "status": "success",
                "documents_processed": len(texts),
                "analysis": extraction_analysis
            }
            
            print(f"âœ… {len(texts)} documents extracted with success")
            print(f"ğŸ“Š Total: {extraction_analysis['total_characters']:,} caractÃ¨res")
            
        except Exception as e:
            results["steps"]["extraction"] = {
                "status": "error",
                "error": str(e)
            }
            print(f"âŒ Error during extraction: {e}")
            return results
        
        # STEP 2: DÃ©coupage en chunks optimisÃ©s
        print(f"\nâœ‚ï¸ STEP 2: DÃ©coupage en chunks optimisÃ©s")
        print("-" * 40)
        
        try:
            chunks = split_texts_into_chunks(
                texts,
                chunk_size_tokens=512,  # Fixed size
                chunk_overlap_tokens=128  # Significant overlap
            )
            
            chunk_analysis = analyze_chunks(chunks)
            
            results["steps"]["chunking"] = {
                "status": "success",
                "chunks_created": len(chunks),
                "analysis": chunk_analysis
            }
            
            print(f"âœ… {len(chunks)} chunks created")
            print(f"ğŸ“Š Taille moyenne: {chunk_analysis['avg_size']:.0f} tokens")
            print(f"ğŸ“ˆ Distribution: {chunk_analysis['size_distribution']}")
            
        except Exception as e:
            results["steps"]["chunking"] = {
                "status": "error",
                "error": str(e)
            }
            print(f"âŒ Erreur lors du chunking: {e}")
            return results
        
        # STEP 3: Embedding Generation
        print(f"\nğŸ§  STEP 3: Embedding Generation ({len(chunks)} chunks)")
        print("-" * 40)
        
        try:
            chunk_embeddings = embed_texts(chunks)
            
            if not chunk_embeddings or not chunk_embeddings[0]:
                raise RuntimeError("No embedding generated")
            
            vector_size = len(chunk_embeddings[0])
            
            results["steps"]["embeddings"] = {
                "status": "success",
                "embeddings_generated": len(chunk_embeddings),
                "vector_dimension": vector_size
            }
            
            print(f"âœ… {len(chunk_embeddings)} embeddings generated")
            print(f"ğŸ“ Vector dimension: {vector_size}")
            
        except Exception as e:
            results["steps"]["embeddings"] = {
                "status": "error",
                "error": str(e)
            }
            print(f"âŒ Error during embedding generation: {e}")
            return results
        
        # STEP 4: Indexation dans Qdrant
        print(f"\nğŸ—„ï¸ STEP 4: Indexation dans la base vectorielle")
        print("-" * 40)
        
        try:
            # CrÃ©er/recrÃ©er la collection
            ensure_collection(self.qdrant_client, vector_size)
            print(f"âœ… Collection '{self.collection_name}' configured")
            
            # Stocker les embeddings
            upsert_embeddings(self.qdrant_client, chunk_embeddings, chunks)
            print(f"âœ… {len(chunks)} chunks indexed in Qdrant")
            
            # VÃ©rifier le stockage
            collection_info = self.qdrant_client.get_collection(self.collection_name)
            points_count = self.qdrant_client.count(self.collection_name).count
            
            results["steps"]["indexing"] = {
                "status": "success",
                "collection_status": collection_info.status,
                "points_stored": points_count,
                "vector_dimension": vector_size
            }
            
            print(f"ğŸ“Š Collection status: {collection_info.status}")
            print(f"ğŸ”¢ Points stored: {points_count}")
            
        except Exception as e:
            results["steps"]["indexing"] = {
                "status": "error",
                "error": str(e)
            }
            print(f"âŒ Error during indexing: {e}")
            return results
        
        # RÃ‰SUMÃ‰ FINAL
        print("\nğŸ‰ INGESTION PIPELINE COMPLETED SUCCESSFULLY!")
        print("=" * 50)
        
        results["status"] = "success"
        results["summary"] = {
            "total_documents": len(texts),
            "total_chunks": len(chunks),
            "total_embeddings": len(chunk_embeddings),
            "total_points_stored": points_count,
            "vector_dimension": vector_size
        }
        
        print(f"ğŸ“ˆ FINAL SUMMARY:")
        print(f"   â€¢ Documents processed: {len(texts)}")
        print(f"   â€¢ Chunks created: {len(chunks)}")
        print(f"   â€¢ Embeddings generated: {len(chunk_embeddings)}")
        print(f"   â€¢ Points indexed: {points_count}")
        print(f"   â€¢ Vector dimension: {vector_size}")
        
        return results
    
    def save_results(self, results: Dict[str, Any], output_file: str = "ingestion_results.json"):
        """Save the pipeline results"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Results saved in {output_file}")
        except Exception as e:
            print(f"âš ï¸  Impossible to save results: {e}")


def main():
    """Main entry point"""
    
    print("ğŸ”§ RAG INGESTION PIPELINE")
    print("=" * 40)
    print("This script processes PDF documents and indexes them into the vector database.")
    print("Run it BEFORE using the Q&A system.\n")
    
    # VÃ©rifier que le dossier data existe
    data_dir = "data"
    if not os.path.exists(data_dir):
        print(f"âŒ The '{data_dir}' directory does not exist.")
        print("Create it and add your PDF documents.")
        return
    
    # VÃ©rifier qu'il y a des PDFs
    pdf_files = [f for f in os.listdir(data_dir) if f.lower().endswith('.pdf')]
    if not pdf_files:
        print(f"âŒ No PDF files found in '{data_dir}'.")
        print("Add PDF documents before running this script.")
        return
    
    print(f"ğŸ“ Data directory: {data_dir}")
    print(f"ğŸ“„ PDFs found: {len(pdf_files)}")
    for pdf in pdf_files[:3]:  # Display the first 3 PDFs
        print(f"   â€¢ {pdf}")
    if len(pdf_files) > 3:
        print(f"   â€¢ ... and {len(pdf_files) - 3} others")
    print()
    
    # Confirmation utilisateur
    response = input("ğŸš€ Start the ingestion pipeline? (y/N): ").strip().lower()
    if response not in ['y', 'yes', 'oui', 'o']:
        print("âŒ Pipeline cancelled.")
        return
    
    # ExÃ©cuter le pipeline
    pipeline = IngestionPipeline(data_dir)
    results = pipeline.run()
    
    # Sauvegarder les rÃ©sultats
    if results.get("status") == "success":
        pipeline.save_results(results)
        print("\nâœ… The RAG system is now ready for questions!")
        print("ğŸ’¡ Use 'python main.py' to ask questions.")
    else:
        print("\nâŒ The pipeline failed. Check the errors above.")


if __name__ == "__main__":
    main()
